<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 tabindex="-1" dir="auto" class=""><a id="user-content-joytag" class="anchor" aria-hidden="true" tabindex="-1" href="#joytag"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欢乐标签</font></font></h1>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 是一种用于标记图像的最先进的人工智能视觉模型，重点关注性积极性和包容性。</font><font style="vertical-align: inherit;">它使用 Danbooru 标记模式，但适用于从手绘到摄影的各种图像。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-info" class="anchor" aria-hidden="true" tabindex="-1" href="#quick-info"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">快速信息</font></font></h2>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下载：</font></font><a href="https://huggingface.co/fancyfeast/joytag/tree/main" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HuggingFace</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">尝试一下：</font></font><a href="https://huggingface.co/spaces/fancyfeast/joytag" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HuggingFace 空间</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">架构：ViT-B/16</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分辨率：448x448x3</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参数：91.5M</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">输出：多标签分类</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">标签： 5000+</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练数据集：Danbooru 2021 + 辅助数据集</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练时间：660M 样本</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F1 分数：0.578 @ 0.4 阈值</font></font></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" tabindex="-1" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例子</font></font></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/fpgaminer/joytag/blob/main/assets/screenshot_20231220a.jpg"><img src="https://github.com/fpgaminer/joytag/raw/main/assets/screenshot_20231220a.jpg" alt="示例1" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/fpgaminer/joytag/blob/main/assets/screenshot_20231220b.jpg"><img src="https://github.com/fpgaminer/joytag/raw/main/assets/screenshot_20231220b.jpg" alt="示例2" style="max-width: 100%;"></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-what-does-it-do" class="anchor" aria-hidden="true" tabindex="-1" href="#what-does-it-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">它有什么作用？</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">向它提供图像，它会输出 5000 多个不同标签的预测。</font><font style="vertical-align: inherit;">它是多标签的，因此每个标签的预测是彼此独立的，这与单类预测视觉模型不同。</font><font style="vertical-align: inherit;">这可以实现图像的自动“标记”，这对于广泛的应用非常有用，包括在缺乏文本对的图像上训练扩散模型。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-using-the-joytag-model" class="anchor" aria-hidden="true" tabindex="-1" href="#using-the-joytag-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 JoyTag 模型</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下载上面指定的模型。</font><font style="vertical-align: inherit;">用法示例：</font></font></p>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">from</span> <span class="pl-v">Models</span> <span class="pl-k">import</span> <span class="pl-v">VisionModel</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>.<span class="pl-s1">amp</span>.<span class="pl-s1">autocast_mode</span>
<span class="pl-k">from</span> <span class="pl-s1">pathlib</span> <span class="pl-k">import</span> <span class="pl-v">Path</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">import</span> <span class="pl-s1">torchvision</span>.<span class="pl-s1">transforms</span>.<span class="pl-s1">functional</span> <span class="pl-k">as</span> <span class="pl-v">TVF</span>

<span class="pl-s1">path</span> <span class="pl-c1">=</span> <span class="pl-s">'/home/.../joytag/models'</span>  <span class="pl-c"># Change this to where you downloaded the model</span>
<span class="pl-v">THRESHOLD</span> <span class="pl-c1">=</span> <span class="pl-c1">0.4</span>

<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">VisionModel</span>.<span class="pl-en">load_model</span>(<span class="pl-s1">path</span>)
<span class="pl-s1">model</span>.<span class="pl-en">eval</span>()
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">to</span>(<span class="pl-s">'cuda'</span>)

<span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-v">Path</span>(<span class="pl-s1">path</span>) <span class="pl-c1">/</span> <span class="pl-s">'top_tags.txt'</span>, <span class="pl-s">'r'</span>) <span class="pl-k">as</span> <span class="pl-s1">f</span>:
	<span class="pl-s1">top_tags</span> <span class="pl-c1">=</span> [<span class="pl-s1">line</span>.<span class="pl-en">strip</span>() <span class="pl-k">for</span> <span class="pl-s1">line</span> <span class="pl-c1">in</span> <span class="pl-s1">f</span>.<span class="pl-en">readlines</span>() <span class="pl-k">if</span> <span class="pl-s1">line</span>.<span class="pl-en">strip</span>()]

<span class="pl-k">def</span> <span class="pl-en">prepare_image</span>(<span class="pl-s1">image</span>: <span class="pl-v">Image</span>.<span class="pl-v">Image</span>, <span class="pl-s1">target_size</span>: <span class="pl-s1">int</span>) <span class="pl-c1">-&gt;</span> <span class="pl-s1">torch</span>.<span class="pl-v">Tensor</span>:
	<span class="pl-c"># Pad image to square</span>
	<span class="pl-s1">image_shape</span> <span class="pl-c1">=</span> <span class="pl-s1">image</span>.<span class="pl-s1">size</span>
	<span class="pl-s1">max_dim</span> <span class="pl-c1">=</span> <span class="pl-en">max</span>(<span class="pl-s1">image_shape</span>)
	<span class="pl-s1">pad_left</span> <span class="pl-c1">=</span> (<span class="pl-s1">max_dim</span> <span class="pl-c1">-</span> <span class="pl-s1">image_shape</span>[<span class="pl-c1">0</span>]) <span class="pl-c1">//</span> <span class="pl-c1">2</span>
	<span class="pl-s1">pad_top</span> <span class="pl-c1">=</span> (<span class="pl-s1">max_dim</span> <span class="pl-c1">-</span> <span class="pl-s1">image_shape</span>[<span class="pl-c1">1</span>]) <span class="pl-c1">//</span> <span class="pl-c1">2</span>

	<span class="pl-s1">padded_image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">new</span>(<span class="pl-s">'RGB'</span>, (<span class="pl-s1">max_dim</span>, <span class="pl-s1">max_dim</span>), (<span class="pl-c1">255</span>, <span class="pl-c1">255</span>, <span class="pl-c1">255</span>))
	<span class="pl-s1">padded_image</span>.<span class="pl-en">paste</span>(<span class="pl-s1">image</span>, (<span class="pl-s1">pad_left</span>, <span class="pl-s1">pad_top</span>))

	<span class="pl-c"># Resize image</span>
	<span class="pl-k">if</span> <span class="pl-s1">max_dim</span> <span class="pl-c1">!=</span> <span class="pl-s1">target_size</span>:
		<span class="pl-s1">padded_image</span> <span class="pl-c1">=</span> <span class="pl-s1">padded_image</span>.<span class="pl-en">resize</span>((<span class="pl-s1">target_size</span>, <span class="pl-s1">target_size</span>), <span class="pl-v">Image</span>.<span class="pl-v">BICUBIC</span>)
	
	<span class="pl-c"># Convert to tensor</span>
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-v">TVF</span>.<span class="pl-en">pil_to_tensor</span>(<span class="pl-s1">padded_image</span>) <span class="pl-c1">/</span> <span class="pl-c1">255.0</span>

	<span class="pl-c"># Normalize</span>
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-v">TVF</span>.<span class="pl-en">normalize</span>(<span class="pl-s1">image_tensor</span>, <span class="pl-s1">mean</span><span class="pl-c1">=</span>[<span class="pl-c1">0.48145466</span>, <span class="pl-c1">0.4578275</span>, <span class="pl-c1">0.40821073</span>], <span class="pl-s1">std</span><span class="pl-c1">=</span>[<span class="pl-c1">0.26862954</span>, <span class="pl-c1">0.26130258</span>, <span class="pl-c1">0.27577711</span>])

	<span class="pl-k">return</span> <span class="pl-s1">image_tensor</span>


<span class="pl-en">@<span class="pl-s1">torch</span>.<span class="pl-en">no_grad</span>()</span>
<span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-s1">image</span>: <span class="pl-v">Image</span>.<span class="pl-v">Image</span>):
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-en">prepare_image</span>(<span class="pl-s1">image</span>, <span class="pl-s1">model</span>.<span class="pl-s1">image_size</span>)
	<span class="pl-s1">batch</span> <span class="pl-c1">=</span> {
		<span class="pl-s">'image'</span>: <span class="pl-s1">image_tensor</span>.<span class="pl-en">unsqueeze</span>(<span class="pl-c1">0</span>).<span class="pl-en">to</span>(<span class="pl-s">'cuda'</span>),
	}

	<span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-s1">amp</span>.<span class="pl-s1">autocast_mode</span>.<span class="pl-en">autocast</span>(<span class="pl-s">'cuda'</span>, <span class="pl-s1">enabled</span><span class="pl-c1">=</span><span class="pl-c1">True</span>):
		<span class="pl-s1">preds</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-s1">batch</span>)
		<span class="pl-s1">tag_preds</span> <span class="pl-c1">=</span> <span class="pl-s1">preds</span>[<span class="pl-s">'tags'</span>].<span class="pl-en">sigmoid</span>().<span class="pl-en">cpu</span>()
	
	<span class="pl-s1">scores</span> <span class="pl-c1">=</span> {<span class="pl-s1">top_tags</span>[<span class="pl-s1">i</span>]: <span class="pl-s1">tag_preds</span>[<span class="pl-c1">0</span>][<span class="pl-s1">i</span>] <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-en">len</span>(<span class="pl-s1">top_tags</span>))}
	<span class="pl-s1">predicted_tags</span> <span class="pl-c1">=</span> [<span class="pl-s1">tag</span> <span class="pl-k">for</span> <span class="pl-s1">tag</span>, <span class="pl-s1">score</span> <span class="pl-c1">in</span> <span class="pl-s1">scores</span>.<span class="pl-en">items</span>() <span class="pl-k">if</span> <span class="pl-s1">score</span> <span class="pl-c1">&gt;</span> <span class="pl-v">THRESHOLD</span>]
	<span class="pl-s1">tag_string</span> <span class="pl-c1">=</span> <span class="pl-s">', '</span>.<span class="pl-en">join</span>(<span class="pl-s1">predicted_tags</span>)

	<span class="pl-k">return</span> <span class="pl-s1">tag_string</span>, <span class="pl-s1">scores</span>

<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'test.jpg'</span>)
<span class="pl-s1">tag_string</span>, <span class="pl-s1">scores</span> <span class="pl-c1">=</span> <span class="pl-en">predict</span>(<span class="pl-s1">image</span>)

<span class="pl-en">print</span>(<span class="pl-s1">tag_string</span>)
<span class="pl-k">for</span> <span class="pl-s1">tag</span>, <span class="pl-s1">score</span> <span class="pl-c1">in</span> <span class="pl-en">sorted</span>(<span class="pl-s1">scores</span>.<span class="pl-en">items</span>(), <span class="pl-s1">key</span><span class="pl-c1">=</span><span class="pl-k">lambda</span> <span class="pl-s1">x</span>: <span class="pl-s1">x</span>[<span class="pl-c1">1</span>], <span class="pl-s1">reverse</span><span class="pl-c1">=</span><span class="pl-c1">True</span>):
	<span class="pl-en">print</span>(<span class="pl-s">f'<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">tag</span><span class="pl-kos">}</span></span>: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">score</span>:.3f<span class="pl-kos">}</span></span>'</span>)</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="from Models import VisionModel
from PIL import Image
import torch.amp.autocast_mode
from pathlib import Path
import torch
import torchvision.transforms.functional as TVF

path = '/home/.../joytag/models'  # Change this to where you downloaded the model
THRESHOLD = 0.4

model = VisionModel.load_model(path)
model.eval()
model = model.to('cuda')

with open(Path(path) / 'top_tags.txt', 'r') as f:
	top_tags = [line.strip() for line in f.readlines() if line.strip()]

def prepare_image(image: Image.Image, target_size: int) -> torch.Tensor:
	# Pad image to square
	image_shape = image.size
	max_dim = max(image_shape)
	pad_left = (max_dim - image_shape[0]) // 2
	pad_top = (max_dim - image_shape[1]) // 2

	padded_image = Image.new('RGB', (max_dim, max_dim), (255, 255, 255))
	padded_image.paste(image, (pad_left, pad_top))

	# Resize image
	if max_dim != target_size:
		padded_image = padded_image.resize((target_size, target_size), Image.BICUBIC)
	
	# Convert to tensor
	image_tensor = TVF.pil_to_tensor(padded_image) / 255.0

	# Normalize
	image_tensor = TVF.normalize(image_tensor, mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])

	return image_tensor


@torch.no_grad()
def predict(image: Image.Image):
	image_tensor = prepare_image(image, model.image_size)
	batch = {
		'image': image_tensor.unsqueeze(0).to('cuda'),
	}

	with torch.amp.autocast_mode.autocast('cuda', enabled=True):
		preds = model(batch)
		tag_preds = preds['tags'].sigmoid().cpu()
	
	scores = {top_tags[i]: tag_preds[0][i] for i in range(len(top_tags))}
	predicted_tags = [tag for tag, score in scores.items() if score > THRESHOLD]
	tag_string = ', '.join(predicted_tags)

	return tag_string, scores

image = Image.open('test.jpg')
tag_string, scores = predict(image)

print(tag_string)
for tag, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
	print(f'{tag}: {score:.3f}')" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-goal" class="anchor" aria-hidden="true" tabindex="-1" href="#goal"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">目标</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大多数公共视觉模型都对其训练数据集进行严格过滤。</font><font style="vertical-align: inherit;">这意味着当今的基础视觉模型在广泛概念的基础层面上表现不佳。</font><font style="vertical-align: inherit;">这限制了言论自由、包容性和多样性。</font><font style="vertical-align: inherit;">它还限制了机器学习模型对我们世界的基本理解。</font><font style="vertical-align: inherit;">JoyTag 团队相信，人类用户应该自由表达自己，而不会因任意和反复无常的内容过滤而受到歧视。</font><font style="vertical-align: inherit;">JoyTag团队还认为，机器学习模型应该对世界有广泛、深入、包容的理解。</font><font style="vertical-align: inherit;">这并不排除使用训练后对齐来减少模型中的偏差，但它确实排除了使用过滤或对齐来降低模型理解世界的能力或用户表达自己的能力。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-the-joytag-model" class="anchor" aria-hidden="true" tabindex="-1" href="#the-joytag-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 模型</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">当前的 JoyTag 模型是在 Danbooru 2021 数据集和一组手动标记图像的组合上进行训练的，以将模型的泛化能力扩展到 danbooru 领域之外。</font><font style="vertical-align: inherit;">Danbooru 数据集因其规模（超过 400 万张人类标记图像）、质量和标签多样性而被用作主要数据源。</font><font style="vertical-align: inherit;">Danbooru 使用的标签系统范围广泛且定义明确。</font><font style="vertical-align: inherit;">然而，Danbooru 数据集的内容多样性有限；</font><font style="vertical-align: inherit;">它主要关注动漫/漫画风格的艺术。</font><font style="vertical-align: inherit;">例如，只有 0.3% 的数据集由摄影图像组成。</font><font style="vertical-align: inherit;">为了解决这个问题，JoyTag 团队手动标记了来自互联网的少量图像，重点关注主数据集中未很好体现的照片和其他内容。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最新发布的模型在包括照片在内的整个数据集上的 F1 得分为 0.578。</font><font style="vertical-align: inherit;">在对训练或验证过程中未见过的图像进行手动测试时，模型表现一致，证明了其良好的泛化能力。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 模型基于 ViT 架构，具有 CNN 干和 GAP 头。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-training-details" class="anchor" aria-hidden="true" tabindex="-1" href="#training-details"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">培训详情</font></font></h2>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">批量大小：4096</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LAMB优化器</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">亚当贝塔：（0.9，0.999）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">亚当·厄普西隆：1e-6</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重量衰减：0.05</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TF32</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FP16混合精度</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">剪辑梯度范数：1.0</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">微不足道的增强</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习率：0.004</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">余弦衰减</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">无需热身</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">没有混淆</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">无标签平滑</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gamma=2.0 时的焦点损失</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以 224x224 分辨率训练 220M 样本，然后以 448x448 分辨率重新启动 440M 样本。</font></font></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-development-notes" class="anchor" aria-hidden="true" tabindex="-1" href="#development-notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">开发笔记</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">此时模型主要受到数据的限制，如果超过当前的训练方案或模型大小，就会发生过度拟合。</font><font style="vertical-align: inherit;">基于 L/14@224 的模型能够在过拟合之前训练到 F1 高达 0.51。</font><font style="vertical-align: inherit;">H/14 也是如此，即使增加了 StochDepth，它也只对 F1 进行了小的改进，然后就过拟合了。</font><font style="vertical-align: inherit;">这是可以预料的，因为 B/16@448 的计算成本与 L/14@224 相似，但参数较少，因此似乎提供了一种在保持良好正则化的同时扩展模型的方法。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在我的 220M 和 440M 规模测试中，MixUp 和 Label 平滑都没有提供改进。</font><font style="vertical-align: inherit;">Mixup 提供了最多的正则化，但最终的 F1 分数低于没有它的情况。</font><font style="vertical-align: inherit;">根据扫描，当前的学习速率、权重衰减和随机深度速率似乎在 220M 范围内接近最佳值。</font><font style="vertical-align: inherit;">没有对训练的 448 部分的设置进行实验，因此那里可能有更优化的学习率。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Trivial 增强比 RandAugment 和其他增强方案效果更好，提供了防止过度拟合的最佳方法。</font><font style="vertical-align: inherit;">其他增强措施可能有助于更多地规范化并允许更多的训练。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">当过度拟合时，模型的 F1 往往会继续提高，而验证损失却会下降。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与普通 BCE 损失相比，焦点损失在 220M 扫描期间似乎没有产生显着差异。</font><font style="vertical-align: inherit;">按标签频率加权并没有改善结果。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">对于稳定的训练来说，损失乘数是必需的，这可能是由于与典型的 BCE 任务相比，标签的数量和极端分布导致损失太低，GradScaler 无法解释。</font><font style="vertical-align: inherit;">只需将损失乘以 128 或 256 就足以稳定训练。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AdamW 的效果也很好，并且可以在 F1 中提供非常小的改进，但 LAMB 会产生更稳定的训练曲线，并且不需要调整热身计划。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 CNN 主干代替 ViT 的传统修补主干为 B/16 模型上的 F1 提供了巨大的提升。</font><font style="vertical-align: inherit;">对于 L/14 型号似乎帮助不大。</font><font style="vertical-align: inherit;">更多的调查可能会取得成果。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-limitations" class="anchor" aria-hidden="true" tabindex="-1" href="#limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">局限性</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">虽然 0.578 的 F1 分数对于像这样高度不平衡的多类任务来说是不错的，但它还远非完美。</font><font style="vertical-align: inherit;">JoyTag 模型比各种 CLIP 等基础模型对 NSFW 概念有更好的理解，但它在细微的概念上失败了，因为它没有足够的数据来表示面部表情（大笑、微笑、咧嘴笑等）。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">有些概念是主观的，因此模型往往会对其产生波动，例如乳房的大小。</font><font style="vertical-align: inherit;">Danbooru 对于如何标记这些内容有明确的指导原则，但数据集本身与这些指导原则不一致。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">缺乏额外的数据意味着当今模型处理不太常见的数据（例如各种类型的服装和时尚）的能力很弱。</font><font style="vertical-align: inherit;">例如，标签“围裙”在辅助性数据集中没有得到很好的表示，因此模型很难处理它。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">像“水印”这样的标签仍然很弱，部分原因是缺乏良好的数据，部分原因是许多水印太小，即使在 448x448 分辨率下模型也无法看到。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danbooru 标签系统主要适用于动漫/漫画风格的艺术，因此，虽然 JoyTag 团队认为该系统足够强大，可以满足大多数摄影内容的需求，但其原始使用指南必须针对摄影内容进行“翻译”，并且模型的需求。</font><font style="vertical-align: inherit;">例如，标签“鼻子”仅被 danbooru 数据集的不到 0.5% 使用，因为它被指定为仅当鼻子比正常情况更“突出”地“绘制”时才使用。</font><font style="vertical-align: inherit;">这将如何转化为摄影内容？</font><font style="vertical-align: inherit;">JoyTag团队已经尽了最大努力来翻译指南，但不平衡和相互冲突的标签规则使得目前这样的标签很麻烦。</font><font style="vertical-align: inherit;">目前，JoyTag 团队倾向于根据简单的规则“可见吗？”来标记照片，这对于模型、训练和下游任务来说被认为更直接。</font><font style="vertical-align: inherit;">“突出”的概念可以通过标签“*_focus”更好地处理。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 团队的目标是让模型以同等的技能处理广泛的内容，并专注于保持辅助数据集的多样性以减少偏差。</font><font style="vertical-align: inherit;">然而，这是一场持续的战斗，永远是一项正在进行的工作。</font><font style="vertical-align: inherit;">例如，该模型仍然难以处理“非常黑的皮肤”等标签。</font><font style="vertical-align: inherit;">如果您发现模型遇到问题的标签，请打开一个问题，让我们了解您的经历，以便我们更好地指导我们的工作。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-metrics" class="anchor" aria-hidden="true" tabindex="-1" href="#metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">指标</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">除了上面报告的平均 F1 之外，还提供了更详细的每个标签指标</font></font><code>full-metrics.txt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">所有指标均在大小为 32,768 的验证集上报告。</font><font style="vertical-align: inherit;">PHash 用于确保验证集排除训练数据中出现的任何图像。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"></font><a href="/fpgaminer/joytag/blob/main/validation-arena"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">验证竞技场</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中提供了与未见过的图像上的其他模型的比较</font><font style="vertical-align: inherit;">。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-future-work" class="anchor" aria-hidden="true" tabindex="-1" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未来的工作</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是初始版本。</font><font style="vertical-align: inherit;">JoyTag 团队正忙于使用新训练的模型来辅助标记更多图像，以扩展辅助数据集并改善模型当前的弱点。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-special-thanks" class="anchor" aria-hidden="true" tabindex="-1" href="#special-thanks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">特别感谢</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">特别感谢</font></font><a href="https://github.com/SmilingWolf/SW-CV-ModelZoo/tree/main"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmilingWolf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，他们在 danbooru 数据集上训练的开创性视觉模型方面所做的工作。</font></font></p>
</article></div>
