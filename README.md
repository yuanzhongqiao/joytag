<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 tabindex="-1" dir="auto"><a id="user-content-joytag" class="anchor" aria-hidden="true" tabindex="-1" href="#joytag"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欢乐标签</font></font></h1>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 是一种用于标记图像的最先进的人工智能视觉模型，重点关注性积极性和包容性。</font><font style="vertical-align: inherit;">它使用 Danbooru 标记模式，但适用于从手绘到摄影的各种图像。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-info" class="anchor" aria-hidden="true" tabindex="-1" href="#quick-info"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">快速信息</font></font></h2>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下载：</font></font><a href="https://huggingface.co/fancyfeast/joytag/tree/main" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HuggingFace</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">尝试一下：</font></font><a href="https://huggingface.co/spaces/fancyfeast/joytag" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HuggingFace 空间</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">架构：ViT-B/16</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分辨率：448x448x3</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参数：91.5M</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">输出：多标签分类</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">标签： 5000+</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练数据集：Danbooru 2021 + 辅助数据集</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练时间：660M 样本</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F1 分数：0.578 @ 0.4 阈值</font></font></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" tabindex="-1" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例子</font></font></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/fpgaminer/joytag/blob/main/assets/screenshot_20231220a.jpg"><img src="https://github.com/fpgaminer/joytag/raw/main/assets/screenshot_20231220a.jpg" alt="示例1" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/fpgaminer/joytag/blob/main/assets/screenshot_20231220b.jpg"><img src="https://github.com/fpgaminer/joytag/raw/main/assets/screenshot_20231220b.jpg" alt="示例2" style="max-width: 100%;"></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-what-does-it-do" class="anchor" aria-hidden="true" tabindex="-1" href="#what-does-it-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">它有什么作用？</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">向它提供图像，它会输出 5000 多个不同标签的预测。</font><font style="vertical-align: inherit;">它是多标签的，因此每个标签的预测是彼此独立的，这与单类预测视觉模型不同。</font><font style="vertical-align: inherit;">这可以实现图像的自动“标记”，这对于广泛的应用非常有用，包括在缺乏文本对的图像上训练扩散模型。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-using-the-joytag-model" class="anchor" aria-hidden="true" tabindex="-1" href="#using-the-joytag-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 JoyTag 模型</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下载上面指定的模型。</font><font style="vertical-align: inherit;">用法示例：</font></font></p>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">from</span> <span class="pl-v">Models</span> <span class="pl-k">import</span> <span class="pl-v">VisionModel</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>.<span class="pl-s1">amp</span>.<span class="pl-s1">autocast_mode</span>
<span class="pl-k">from</span> <span class="pl-s1">pathlib</span> <span class="pl-k">import</span> <span class="pl-v">Path</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">import</span> <span class="pl-s1">torchvision</span>.<span class="pl-s1">transforms</span>.<span class="pl-s1">functional</span> <span class="pl-k">as</span> <span class="pl-v">TVF</span>

<span class="pl-s1">path</span> <span class="pl-c1">=</span> <span class="pl-s">'/home/.../joytag/models'</span>  <span class="pl-c"># Change this to where you downloaded the model</span>
<span class="pl-v">THRESHOLD</span> <span class="pl-c1">=</span> <span class="pl-c1">0.4</span>

<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">VisionModel</span>.<span class="pl-en">load_model</span>(<span class="pl-s1">path</span>)
<span class="pl-s1">model</span>.<span class="pl-en">eval</span>()
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">to</span>(<span class="pl-s">'cuda'</span>)

<span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-v">Path</span>(<span class="pl-s1">path</span>) <span class="pl-c1">/</span> <span class="pl-s">'top_tags.txt'</span>, <span class="pl-s">'r'</span>) <span class="pl-k">as</span> <span class="pl-s1">f</span>:
	<span class="pl-s1">top_tags</span> <span class="pl-c1">=</span> [<span class="pl-s1">line</span>.<span class="pl-en">strip</span>() <span class="pl-k">for</span> <span class="pl-s1">line</span> <span class="pl-c1">in</span> <span class="pl-s1">f</span>.<span class="pl-en">readlines</span>() <span class="pl-k">if</span> <span class="pl-s1">line</span>.<span class="pl-en">strip</span>()]

<span class="pl-k">def</span> <span class="pl-en">prepare_image</span>(<span class="pl-s1">image</span>: <span class="pl-v">Image</span>.<span class="pl-v">Image</span>, <span class="pl-s1">target_size</span>: <span class="pl-s1">int</span>) <span class="pl-c1">-&gt;</span> <span class="pl-s1">torch</span>.<span class="pl-v">Tensor</span>:
	<span class="pl-c"># Pad image to square</span>
	<span class="pl-s1">image_shape</span> <span class="pl-c1">=</span> <span class="pl-s1">image</span>.<span class="pl-s1">size</span>
	<span class="pl-s1">max_dim</span> <span class="pl-c1">=</span> <span class="pl-en">max</span>(<span class="pl-s1">image_shape</span>)
	<span class="pl-s1">pad_left</span> <span class="pl-c1">=</span> (<span class="pl-s1">max_dim</span> <span class="pl-c1">-</span> <span class="pl-s1">image_shape</span>[<span class="pl-c1">0</span>]) <span class="pl-c1">//</span> <span class="pl-c1">2</span>
	<span class="pl-s1">pad_top</span> <span class="pl-c1">=</span> (<span class="pl-s1">max_dim</span> <span class="pl-c1">-</span> <span class="pl-s1">image_shape</span>[<span class="pl-c1">1</span>]) <span class="pl-c1">//</span> <span class="pl-c1">2</span>

	<span class="pl-s1">padded_image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">new</span>(<span class="pl-s">'RGB'</span>, (<span class="pl-s1">max_dim</span>, <span class="pl-s1">max_dim</span>), (<span class="pl-c1">255</span>, <span class="pl-c1">255</span>, <span class="pl-c1">255</span>))
	<span class="pl-s1">padded_image</span>.<span class="pl-en">paste</span>(<span class="pl-s1">image</span>, (<span class="pl-s1">pad_left</span>, <span class="pl-s1">pad_top</span>))

	<span class="pl-c"># Resize image</span>
	<span class="pl-k">if</span> <span class="pl-s1">max_dim</span> <span class="pl-c1">!=</span> <span class="pl-s1">target_size</span>:
		<span class="pl-s1">padded_image</span> <span class="pl-c1">=</span> <span class="pl-s1">padded_image</span>.<span class="pl-en">resize</span>((<span class="pl-s1">target_size</span>, <span class="pl-s1">target_size</span>), <span class="pl-v">Image</span>.<span class="pl-v">BICUBIC</span>)
	
	<span class="pl-c"># Convert to tensor</span>
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-v">TVF</span>.<span class="pl-en">pil_to_tensor</span>(<span class="pl-s1">padded_image</span>) <span class="pl-c1">/</span> <span class="pl-c1">255.0</span>

	<span class="pl-c"># Normalize</span>
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-v">TVF</span>.<span class="pl-en">normalize</span>(<span class="pl-s1">image_tensor</span>, <span class="pl-s1">mean</span><span class="pl-c1">=</span>[<span class="pl-c1">0.48145466</span>, <span class="pl-c1">0.4578275</span>, <span class="pl-c1">0.40821073</span>], <span class="pl-s1">std</span><span class="pl-c1">=</span>[<span class="pl-c1">0.26862954</span>, <span class="pl-c1">0.26130258</span>, <span class="pl-c1">0.27577711</span>])

	<span class="pl-k">return</span> <span class="pl-s1">image_tensor</span>


<span class="pl-en">@<span class="pl-s1">torch</span>.<span class="pl-en">no_grad</span>()</span>
<span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-s1">image</span>: <span class="pl-v">Image</span>.<span class="pl-v">Image</span>):
	<span class="pl-s1">image_tensor</span> <span class="pl-c1">=</span> <span class="pl-en">prepare_image</span>(<span class="pl-s1">image</span>, <span class="pl-s1">model</span>.<span class="pl-s1">image_size</span>)
	<span class="pl-s1">batch</span> <span class="pl-c1">=</span> {
		<span class="pl-s">'image'</span>: <span class="pl-s1">image_tensor</span>.<span class="pl-en">unsqueeze</span>(<span class="pl-c1">0</span>).<span class="pl-en">to</span>(<span class="pl-s">'cuda'</span>),
	}

	<span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-s1">amp</span>.<span class="pl-s1">autocast_mode</span>.<span class="pl-en">autocast</span>(<span class="pl-s">'cuda'</span>, <span class="pl-s1">enabled</span><span class="pl-c1">=</span><span class="pl-c1">True</span>):
		<span class="pl-s1">preds</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-s1">batch</span>)
		<span class="pl-s1">tag_preds</span> <span class="pl-c1">=</span> <span class="pl-s1">preds</span>[<span class="pl-s">'tags'</span>].<span class="pl-en">sigmoid</span>().<span class="pl-en">cpu</span>()
	
	<span class="pl-s1">scores</span> <span class="pl-c1">=</span> {<span class="pl-s1">top_tags</span>[<span class="pl-s1">i</span>]: <span class="pl-s1">tag_preds</span>[<span class="pl-c1">0</span>][<span class="pl-s1">i</span>] <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-en">len</span>(<span class="pl-s1">top_tags</span>))}
	<span class="pl-s1">predicted_tags</span> <span class="pl-c1">=</span> [<span class="pl-s1">tag</span> <span class="pl-k">for</span> <span class="pl-s1">tag</span>, <span class="pl-s1">score</span> <span class="pl-c1">in</span> <span class="pl-s1">scores</span>.<span class="pl-en">items</span>() <span class="pl-k">if</span> <span class="pl-s1">score</span> <span class="pl-c1">&gt;</span> <span class="pl-v">THRESHOLD</span>]
	<span class="pl-s1">tag_string</span> <span class="pl-c1">=</span> <span class="pl-s">', '</span>.<span class="pl-en">join</span>(<span class="pl-s1">predicted_tags</span>)

	<span class="pl-k">return</span> <span class="pl-s1">tag_string</span>, <span class="pl-s1">scores</span>

<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s">'test.jpg'</span>)
<span class="pl-s1">tag_string</span>, <span class="pl-s1">scores</span> <span class="pl-c1">=</span> <span class="pl-en">predict</span>(<span class="pl-s1">image</span>)

<span class="pl-en">print</span>(<span class="pl-s1">tag_string</span>)
<span class="pl-k">for</span> <span class="pl-s1">tag</span>, <span class="pl-s1">score</span> <span class="pl-c1">in</span> <span class="pl-en">sorted</span>(<span class="pl-s1">scores</span>.<span class="pl-en">items</span>(), <span class="pl-s1">key</span><span class="pl-c1">=</span><span class="pl-k">lambda</span> <span class="pl-s1">x</span>: <span class="pl-s1">x</span>[<span class="pl-c1">1</span>], <span class="pl-s1">reverse</span><span class="pl-c1">=</span><span class="pl-c1">True</span>):
	<span class="pl-en">print</span>(<span class="pl-s">f'<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">tag</span><span class="pl-kos">}</span></span>: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">score</span>:.3f<span class="pl-kos">}</span></span>'</span>)</pre><div class="zeroclipboard-container">
 
  </div></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-goal" class="anchor" aria-hidden="true" tabindex="-1" href="#goal"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">目标</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大多数公共视觉模型都对其训练数据集进行严格过滤。</font><font style="vertical-align: inherit;">这意味着当今的基础视觉模型在广泛概念的基础层面上表现不佳。</font><font style="vertical-align: inherit;">这限制了言论自由、包容性和多样性。</font><font style="vertical-align: inherit;">它还限制了机器学习模型对我们世界的基本理解。</font><font style="vertical-align: inherit;">JoyTag 团队相信，人类用户应该自由表达自己，而不会因任意和反复无常的内容过滤而受到歧视。</font><font style="vertical-align: inherit;">JoyTag团队还认为，机器学习模型应该对世界有广泛、深入、包容的理解。</font><font style="vertical-align: inherit;">这并不排除使用训练后对齐来减少模型中的偏差，但它确实排除了使用过滤或对齐来降低模型理解世界的能力或用户表达自己的能力。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-the-joytag-model" class="anchor" aria-hidden="true" tabindex="-1" href="#the-joytag-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 模型</font></font></h2>
<p dir="auto">The current JoyTag model is trained on a combination of the Danbooru 2021 dataset alongside a set of manually tagged images to expand the model's generalization beyond the danbooru domain.  The Danbooru dataset is used as the primary source of data because of its size (over 4 million human tagged images), its quality, and its diversity of tags.  The tagging system used by Danbooru is wide ranging and well defined.  However, the Danbooru dataset is limited in its diversity of content; it primarily focusses on anime/manga style art.  For example, only 0.3% of the dataset consists of photographic images.  To address this, the JoyTag team manually tagged a small number of images from the internet with a focus on photographs and other content not well represented in the primary dataset.</p>
<p dir="auto">The latest model release achieves an F1 score of 0.578 across the entire dataset including photographs.  In manual testing on images not seen during training or validation, the model performs consistently, proving its ability to generalize well.</p>
<p dir="auto">The JoyTag model is based on the ViT architecture with a CNN stem and GAP head.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-training-details" class="anchor" aria-hidden="true" tabindex="-1" href="#training-details"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training details</h2>
<ul dir="auto">
<li>Batch Size: 4096</li>
<li>LAMB Optimizer</li>
<li>Adam Beta: (0.9, 0.999)</li>
<li>Adam Epsilon: 1e-6</li>
<li>Weight Decay: 0.05</li>
<li>TF32</li>
<li>FP16 mixed precision</li>
<li>Clip Gradient Norm: 1.0</li>
<li>Trivial Augment</li>
<li>Learning Rate: 0.004</li>
<li>Cosine Decay</li>
<li>No Warmup</li>
<li>No mixup</li>
<li>No label smoothing</li>
<li>Focal loss with gamma=2.0</li>
<li>Trained for 220M samples at a resolution of 224x224, followed by a restart for 440M samples at a resolution of 448x448.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-development-notes" class="anchor" aria-hidden="true" tabindex="-1" href="#development-notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Development Notes</h2>
<p dir="auto">The model is primarily limited by data at this time, with overfitting occurring if pushed past the current training regimen or model size.  L/14@224 based models are able to train up to an F1 of 0.51 before overfitting.  Same for H/14 which only made a small F1 improvement and then overfit, even with increased StochDepth.  This is to be expected, as B/16@448 has similar computational costs as L/14@224, but less parameters, and thus seems to provide a way to expand the model while keeping good regularization.</p>
<p dir="auto">Neither MixUp nor Label smoothing provided improvements in my testings at both 220M and 440M scales.  Mixup provided the most regularization, but the end F1 score was lower than without it.  The current learning rate, weight decay, and stochdepth rates appear to be close to optimal in the 220M range based on sweeps.  No experimentation was done on the settings for the 448 section of training, so its possible a more optimal learning rate is available there.</p>
<p dir="auto">Trivial augment worked better than RandAugment and other augmentation schemes, providing the best cure against overfitting.  It's possible other augmentations could help regularize more and allow more training.</p>
<p dir="auto">When overfitting, the model's F1 tends to continue to improve while the validation loss tanks.</p>
<p dir="auto">Focal loss did not seem to make a significant difference during 220M sweeps, comparing to plain BCE loss.  Weighting by tag frequency did not improve results.</p>
<p dir="auto">A loss multiplier was mandatory for stable training, likely due to quantity and extreme distribution of the tags compared to typical BCE tasks causing the loss to be too low for GradScaler to account for.  Simply multiplying the loss by 128 or 256 was sufficient to stabilize training.</p>
<p dir="auto">AdamW works as well and can provide a very small improvement in F1, but LAMB results in a more stable training curve, and doesn't require tuning the warmup schedule.</p>
<p dir="auto">Using a CNN stem instead of the traditional patching stem of ViTs provided a large boost to F1 on B/16 models.  It seemed to help less for L/14 models.  More investigation might be fruitful.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-limitations" class="anchor" aria-hidden="true" tabindex="-1" href="#limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Limitations</h2>
<p dir="auto">While an F1 score of 0.578 is good for a highly imbalanced multi-class task such as this, it is far from perfect.  The JoyTag model has a far better understanding of NSFW concepts than foundational models like the various CLIPs, but it fails on nuanced concepts that it just doesn't have enough data for like facial expressions (laughing, smiling, grinning, etc).</p>
<p dir="auto">Some concepts are subjective, and thus the model tends to vasilate on them, like the size of breasts.  Danbooru has well specified guidelines for how these are to be tagged, but the dataset itself is not consistent with these guidelines.</p>
<p dir="auto">A lack of additional data means that less common data, like various types of clothing and fashion, are handled weakly by the model today.  For example the tag "apron" is not well represented in the auxility dataset, and thus the model struggles with it.</p>
<p dir="auto">Tags like "watermark" are still weak, in part because of a lack of good data, and in part because many watermarks are too small to be seen by the model even at 448x448 resolution.</p>
<p dir="auto">The Danbooru tagging system is primarily geared for anime/manga style art, so while the JoyTag team believes that the system is robust enough to handle the needs of most photographic content, the original guidelines of its usage must be "translated" for photographic content and the needs of the model.  For example, the tag "nose" is only used by less than 0.5% of the danbooru dataset, because it is specified as being used only when the nose is "drawn" more "prominently" than normal.  How would this translate to photographic content?  The JoyTag team has made its best effort to translate the guidelines, but the imbalance and conflicting tagging rules make tags like this troublesome at this time.  For now, the JoyTag team has leaned towards tagging photographs based on the simple rule "is it visible?", which is believed to be more straightforward for the model, training, and downstream tasks.  The concept of "prominent" is better handled by tags ala "*_focus".</p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JoyTag 团队的目标是让模型以同等的技能处理广泛的内容，并专注于保持辅助数据集的多样性以减少偏差。</font><font style="vertical-align: inherit;">然而，这是一场持续的战斗，永远是一项正在进行的工作。</font><font style="vertical-align: inherit;">例如，该模型仍然难以处理“非常黑的皮肤”等标签。</font><font style="vertical-align: inherit;">如果您发现模型遇到问题的标签，请打开一个问题，让我们了解您的经历，以便我们更好地指导我们的工作。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-metrics" class="anchor" aria-hidden="true" tabindex="-1" href="#metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">指标</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">除了上面报告的平均 F1 之外，还提供了更详细的每个标签指标</font></font><code>full-metrics.txt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">所有指标均在大小为 32,768 的验证集上报告。</font><font style="vertical-align: inherit;">PHash 用于确保验证集排除训练数据中出现的任何图像。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"></font><a href="/fpgaminer/joytag/blob/main/validation-arena"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">验证竞技场</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中提供了与未见过的图像上的其他模型的比较</font><font style="vertical-align: inherit;">。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-future-work" class="anchor" aria-hidden="true" tabindex="-1" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未来的工作</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是初始版本。</font><font style="vertical-align: inherit;">JoyTag 团队正忙于使用新训练的模型来辅助标记更多图像，以扩展辅助数据集并改善模型当前的弱点。</font></font></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-special-thanks" class="anchor" aria-hidden="true" tabindex="-1" href="#special-thanks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">特别感谢</font></font></h2>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">特别感谢</font></font><a href="https://github.com/SmilingWolf/SW-CV-ModelZoo/tree/main"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmilingWolf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，他们在 danbooru 数据集上训练的开创性视觉模型方面所做的工作。</font></font></p>
</article></div>
